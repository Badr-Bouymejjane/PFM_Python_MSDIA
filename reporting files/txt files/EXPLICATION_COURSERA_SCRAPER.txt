================================================================================
   EXPLICATION DÃ‰TAILLÃ‰E DU CODE : coursera_scraper.py
================================================================================

Ce document explique Ã©tape par Ã©tape le fonctionnement du scraper Coursera
sans dÃ©tailler les fonctions individuelles.

================================================================================
ğŸ“‹ Ã‰TAPE 1 : IMPORTATION DES MODULES (lignes 6-18)
================================================================================

Le code commence par importer tous les modules nÃ©cessaires :

â€¢ MODULES SYSTÃˆME (sys, os) : pour gÃ©rer les chemins de fichiers et ajouter
  le rÃ©pertoire parent au chemin de recherche des modules

â€¢ BIBLIOTHÃˆQUES PRINCIPALES :
  - asyncio : pour la programmation asynchrone (permet d'exÃ©cuter plusieurs
    tÃ¢ches en parallÃ¨le sans bloquer le programme)
  - pandas : pour manipuler les donnÃ©es sous forme de tableaux (DataFrames)
  - playwright : pour automatiser un navigateur web et scraper du contenu
    dynamique (JavaScript)
  - re : pour utiliser des expressions rÃ©guliÃ¨res (recherche de patterns
    dans le texte)
  - time, datetime : pour gÃ©rer le temps et les dates

================================================================================
ğŸ”¹ Ã‰TAPE 2 : CHARGEMENT DE LA CONFIGURATION (lignes 20-29)
================================================================================

Le code essaie d'importer des paramÃ¨tres depuis un fichier config.py :

â€¢ SI LE FICHIER EXISTE â†’ utilise les paramÃ¨tres dÃ©finis dans config.py

â€¢ SI LE FICHIER N'EXISTE PAS â†’ utilise des valeurs par dÃ©faut :
  - CATEGORIES : liste des catÃ©gories Ã  scraper
    Exemple: ['data-science', 'machine-learning', 'python']
  - MAX_COURSES_PER_CATEGORY : nombre maximum de cours Ã  extraire par
    catÃ©gorie (30 par dÃ©faut)
  - HEADLESS_MODE : mode invisible du navigateur
    (True = pas d'interface graphique visible)
  - REQUEST_DELAY : dÃ©lai entre les requÃªtes (2 secondes pour Ã©viter de
    surcharger le serveur et Ã©viter d'Ãªtre bloquÃ©)

================================================================================
ğŸ”¹ Ã‰TAPE 3 : DÃ‰FINITION DE LA CLASSE CourseraScraper (lignes 32-43)
================================================================================

Une classe est crÃ©Ã©e pour organiser tout le code du scraper de maniÃ¨re
structurÃ©e et rÃ©utilisable.

â€¢ ATTRIBUTS DE CLASSE (lignes 37-38) :
  - BASE_URL : URL de base de Coursera (https://www.coursera.org)
  - SEARCH_URL : template d'URL pour effectuer des recherches
    Format: https://www.coursera.org/search?query={query}

â€¢ CONSTRUCTEUR __init__ (lignes 40-43) :
  - Initialise le mode d'affichage du navigateur (headless)
  - CrÃ©e une liste vide self.courses pour stocker tous les cours extraits

================================================================================
ğŸ”¹ Ã‰TAPE 4 : INITIALISATION DU NAVIGATEUR (lignes 45-57)
================================================================================

La mÃ©thode init_browser() dÃ©marre un navigateur automatisÃ© avec Playwright :

1. DÃ‰MARRE PLAYWRIGHT (ligne 48)
   - Lance le framework d'automatisation de navigateur

2. LANCE LE NAVIGATEUR CHROMIUM (ligne 50)
   - En mode headless (invisible) ou visible selon la configuration

3. CRÃ‰E UN CONTEXTE DE NAVIGATION (lignes 52-55)
   - RÃ©solution Full HD (1920x1080) pour voir tout le contenu
   - User-Agent qui simule un vrai navigateur Windows
     (pour Ã©viter d'Ãªtre dÃ©tectÃ© comme un bot)

4. OUVRE UNE NOUVELLE PAGE (ligne 57)
   - CrÃ©e un onglet dans le contexte de navigation

================================================================================
ğŸ”¹ Ã‰TAPE 5 : FERMETURE DU NAVIGATEUR (lignes 59-62)
================================================================================

La mÃ©thode close_browser() ferme proprement le navigateur et arrÃªte Playwright
pour libÃ©rer les ressources systÃ¨me.

================================================================================
ğŸ”¹ Ã‰TAPE 6 : GESTION DU POPUP DE COOKIES (lignes 64-85)
================================================================================

La mÃ©thode handle_cookie_consent() cherche et clique sur le bouton
d'acceptation des cookies :

â€¢ STRATÃ‰GIE : Essaie plusieurs sÃ©lecteurs CSS diffÃ©rents car le bouton peut
  avoir diffÃ©rents formats selon la langue ou la version du site :
  - Bouton avec texte "Accept" (anglais)
  - Bouton avec texte "Accepter" (franÃ§ais)
  - Bouton avec attribut data-testid="accept-cookies"
  - Bouton avec ID #onetrust-accept-btn-handler

â€¢ COMPORTEMENT :
  - Si un bouton est trouvÃ© â†’ clique dessus et attend 1 seconde
  - Si aucun bouton n'est trouvÃ© â†’ ignore (pas de popup sur cette page)

================================================================================
ğŸ”¹ Ã‰TAPE 7 : DÃ‰FILEMENT DE LA PAGE (lignes 87-95)
================================================================================

La mÃ©thode scroll_page() fait dÃ©filer la page plusieurs fois :

â€¢ POURQUOI ? Coursera utilise le "lazy loading" : le contenu se charge
  dynamiquement au fur et Ã  mesure du dÃ©filement pour Ã©conomiser la bande
  passante

â€¢ FONCTIONNEMENT :
  - ExÃ©cute du JavaScript pour dÃ©filer d'une hauteur d'Ã©cran Ã  chaque fois
  - Attend 1 seconde entre chaque dÃ©filement pour laisser le contenu se charger
  - Par dÃ©faut, fait 5 dÃ©filements (paramÃ¨tre scroll_count=5)

================================================================================
ğŸ”¹ Ã‰TAPE 8 : EXTRACTION SÃ‰CURISÃ‰E DE TEXTE (lignes 97-102)
================================================================================

La mÃ©thode extract_text() extrait du texte d'un Ã©lÃ©ment de maniÃ¨re sÃ©curisÃ©e :

â€¢ Supprime les espaces inutiles au dÃ©but et Ã  la fin avec .strip()
â€¢ Retourne une valeur par dÃ©faut si l'Ã©lÃ©ment n'existe pas ou est vide
â€¢ GÃ¨re les exceptions pour Ã©viter que le programme plante

================================================================================
ğŸ”¹ Ã‰TAPE 9 : SCRAPING D'UNE CATÃ‰GORIE (lignes 104-149)
================================================================================

La mÃ©thode scrape_category() extrait les cours d'une catÃ©gorie donnÃ©e :

1. CONSTRUIT L'URL DE RECHERCHE (ligne 108)
   - Remplace les tirets par des espaces dans la catÃ©gorie
   - Exemple: 'data-science' devient 'data science'

2. NAVIGUE VERS LA PAGE (ligne 111)
   - Attend que le rÃ©seau soit inactif (networkidle) pour s'assurer que
     tout le contenu est chargÃ©
   - Timeout de 30 secondes maximum

3. GÃˆRE LE POPUP DE COOKIES (ligne 112)
   - Appelle la mÃ©thode handle_cookie_consent()

4. FAIT DÃ‰FILER LA PAGE (ligne 114)
   - Appelle scroll_page(5) pour charger plus de contenu

5. CHERCHE LES CARTES DE COURS (lignes 116-129)
   - Essaie plusieurs sÃ©lecteurs CSS possibles car la structure HTML peut
     varier
   - DÃ¨s qu'un sÃ©lecteur trouve des cartes, arrÃªte la recherche

6. EXTRAIT LES DONNÃ‰ES DE CHAQUE CARTE (lignes 136-142)
   - Boucle sur chaque carte trouvÃ©e (limitÃ© Ã  max_courses)
   - Appelle extract_course_data() pour chaque carte
   - Ignore les erreurs et continue avec la carte suivante

7. RETOURNE LA LISTE DES COURS (ligne 145)
   - Retourne tous les cours extraits de cette catÃ©gorie

================================================================================
ğŸ”¹ Ã‰TAPE 10 : EXTRACTION DES DONNÃ‰ES D'UN COURS (lignes 151-292)
================================================================================

La mÃ©thode extract_course_data() extrait toutes les informations d'une carte
de cours. Elle crÃ©e un dictionnaire avec tous les champs suivants :

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STRUCTURE DU DICTIONNAIRE CRÃ‰Ã‰ (lignes 154-158)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â€¢ platform : "Coursera" (nom de la plateforme)
â€¢ category : catÃ©gorie formatÃ©e (ex: "Data Science")
â€¢ scraped_at : date/heure d'extraction au format ISO
  (ex: '2026-02-13T09:30:00')

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ EXTRACTION DES CHAMPS (avec plusieurs sÃ©lecteurs CSS pour chaque)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âŠ TITRE (lignes 160-170)
   - Cherche dans les balises <h3>, <h2>, ou attributs spÃ©cifiques
   - Extrait le texte du premier Ã©lÃ©ment trouvÃ©

â‹ INSTRUCTEUR/ORGANISATION (lignes 172-181)
   - Cherche le nom de l'universitÃ© ou de l'instructeur
   - Utilise des sÃ©lecteurs spÃ©cifiques Ã  Coursera

âŒ NOTE - RATING (lignes 183-199)
   - Extrait le nombre d'Ã©toiles (ex: "4.8 stars")
   - Utilise une expression rÃ©guliÃ¨re pour extraire le nombre
   - Convertit en float (nombre dÃ©cimal)

âŒ NOMBRE DE REVIEWS (lignes 201-218)
   - Extrait le nombre d'avis (ex: "22,772 reviews" ou "150K students")
   - GÃ¨re les formats avec suffixes K (milliers) et M (millions)
   - Convertit "22K" en 22000, "1.5M" en 1500000

â NIVEAU (lignes 220-239)
   - DÃ©tecte si c'est "Beginner", "Intermediate" ou "Advanced"
   - Utilise des mots-clÃ©s en franÃ§ais et anglais :
     * Beginner: dÃ©butant, beginner, introduct, basic
     * Intermediate: intermÃ©diaire, intermediate, medium
     * Advanced: avancÃ©, advanced, expert, professional

â PRIX (lignes 241-257)
   - DÃ©tecte si le cours est gratuit (mots "free" ou "gratuit")
   - Cherche un prix avec symbole monÃ©taire (ex: "$49.99", "â‚¬29.99")
   - Si pas de prix trouvÃ©, marque comme "Subscription"

â URL (lignes 259-271)
   - Extrait le lien href du premier lien <a> dans la carte
   - Si l'URL est relative (commence par /), ajoute BASE_URL
   - Sinon, utilise l'URL telle quelle

â‘ DESCRIPTION (lignes 273-284)
   - Cherche les paragraphes de description
   - Si pas de description trouvÃ©e, utilise le titre comme description

â’ INFORMATIONS SUPPLÃ‰MENTAIRES (lignes 286-290)
   - skills : utilise la catÃ©gorie comme compÃ©tences
     (ex: "data-science" devient "data, science")
   - language : par dÃ©faut "English" (Coursera est principalement en anglais)

â“ RETOURNE LE DICTIONNAIRE (ligne 292)
   - Retourne le dictionnaire complet avec toutes les donnÃ©es du cours

================================================================================
ğŸ”¹ Ã‰TAPE 11 : SCRAPING DE TOUTES LES CATÃ‰GORIES (lignes 294-327)
================================================================================

La mÃ©thode scrape_all() orchestre le scraping complet de toutes les
catÃ©gories :

1. UTILISE LES CATÃ‰GORIES DE LA CONFIG (lignes 297-298)
   - Si aucune catÃ©gorie n'est fournie en paramÃ¨tre, utilise celles
     dÃ©finies dans la configuration

2. AFFICHE UN HEADER (lignes 300-303)
   - Affiche un en-tÃªte visuel pour indiquer le dÃ©but du scraping

3. INITIALISE LE NAVIGATEUR (ligne 306)
   - Appelle init_browser() pour dÃ©marrer Playwright

4. BOUCLE SUR CHAQUE CATÃ‰GORIE (lignes 310-318)
   - Pour chaque catÃ©gorie dans la liste :
     * Affiche la progression [1/3], [2/3], etc.
     * Scrape la catÃ©gorie avec scrape_category()
     * Ajoute les cours Ã  la liste totale avec extend()
     * Attend REQUEST_DELAY secondes avant la suivante (Ã©vite la surcharge)

5. FERME LE NAVIGATEUR (ligne 321)
   - Appelle close_browser() pour libÃ©rer les ressources

6. STOCKE LES COURS (ligne 324)
   - Stocke tous les cours dans l'attribut self.courses

7. RETOURNE TOUS LES COURS (ligne 327)
   - Retourne la liste complÃ¨te de tous les cours de toutes les catÃ©gories

================================================================================
ğŸ”¹ Ã‰TAPE 12 : CONVERSION EN DATAFRAME (lignes 329-333)
================================================================================

La mÃ©thode to_dataframe() convertit la liste de dictionnaires en DataFrame
Pandas :

â€¢ Chaque dictionnaire (cours) devient une ligne du tableau
â€¢ Les clÃ©s des dictionnaires deviennent les colonnes
â€¢ Permet de manipuler les donnÃ©es facilement (filtrage, tri, statistiques)

================================================================================
ğŸ”¹ Ã‰TAPE 13 : SAUVEGARDE EN CSV (lignes 335-342)
================================================================================

La mÃ©thode save_to_csv() exporte les donnÃ©es en fichier CSV :

1. Convertit la liste de cours en DataFrame avec to_dataframe()
2. Sauvegarde le DataFrame en fichier CSV avec :
   - index=False : n'inclut pas l'index des lignes (0, 1, 2, ...)
   - encoding='utf-8' : utilise l'encodage UTF-8 pour supporter les
     caractÃ¨res spÃ©ciaux (accents, Ã©mojis, etc.)
3. Affiche un message de confirmation avec le chemin du fichier

================================================================================
ğŸ”¹ Ã‰TAPE 14 : FONCTION PRINCIPALE DE TEST (lignes 345-358)
================================================================================

La fonction main() teste le scraper avec un exemple concret :

1. CRÃ‰E UNE INSTANCE DU SCRAPER (ligne 347)
   - Utilise le mode headless dÃ©fini dans la configuration

2. SCRAPE 3 CATÃ‰GORIES DE TEST (ligne 351)
   - CatÃ©gories: 'data-science', 'machine-learning', 'python'
   - Maximum 20 cours par catÃ©gorie

3. SAUVEGARDE LES RÃ‰SULTATS (ligne 354)
   - Enregistre dans 'data/coursera_courses.csv'

4. AFFICHE UN APERÃ‡U (lignes 355-357)
   - Convertit en DataFrame et affiche les 5 premiÃ¨res lignes avec head()

================================================================================
ğŸ”¹ Ã‰TAPE 15 : POINT D'ENTRÃ‰E DU PROGRAMME (lignes 360-361)
================================================================================

if __name__ == "__main__":
    asyncio.run(main())

â€¢ SI LE FICHIER EST EXÃ‰CUTÃ‰ DIRECTEMENT (python coursera_scraper.py)
  â†’ Lance la fonction main() de maniÃ¨re asynchrone avec asyncio.run()

â€¢ SI LE FICHIER EST IMPORTÃ‰ (import coursera_scraper)
  â†’ Ne fait rien, permet d'utiliser la classe CourseraScraper dans
    d'autres fichiers sans exÃ©cuter le test

================================================================================
ğŸ¯ RÃ‰SUMÃ‰ DU FLUX D'EXÃ‰CUTION COMPLET
================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. CONFIGURATION                                                        â”‚
â”‚    â””â”€ Charge les paramÃ¨tres (catÃ©gories, limites, dÃ©lais)              â”‚
â”‚                                                                          â”‚
â”‚ 2. INITIALISATION                                                       â”‚
â”‚    â””â”€ DÃ©marre le navigateur Playwright avec Chromium                   â”‚
â”‚                                                                          â”‚
â”‚ 3. POUR CHAQUE CATÃ‰GORIE                                               â”‚
â”‚    â”œâ”€ Navigue vers la page de recherche Coursera                       â”‚
â”‚    â”œâ”€ GÃ¨re les popups de cookies                                       â”‚
â”‚    â”œâ”€ Fait dÃ©filer pour charger le contenu dynamique                   â”‚
â”‚    â”œâ”€ Extrait les cartes de cours                                      â”‚
â”‚    â””â”€ POUR CHAQUE CARTE                                                â”‚
â”‚       â”œâ”€ Extrait le titre                                              â”‚
â”‚       â”œâ”€ Extrait l'instructeur/organisation                            â”‚
â”‚       â”œâ”€ Extrait la note et le nombre de reviews                       â”‚
â”‚       â”œâ”€ Extrait le niveau (Beginner/Intermediate/Advanced)            â”‚
â”‚       â”œâ”€ Extrait le prix (Free/Payant/Subscription)                    â”‚
â”‚       â”œâ”€ Extrait l'URL du cours                                        â”‚
â”‚       â”œâ”€ Extrait la description                                        â”‚
â”‚       â””â”€ Ajoute les compÃ©tences et la langue                           â”‚
â”‚                                                                          â”‚
â”‚ 4. FERMETURE                                                            â”‚
â”‚    â””â”€ Ferme le navigateur et libÃ¨re les ressources                     â”‚
â”‚                                                                          â”‚
â”‚ 5. SAUVEGARDE                                                           â”‚
â”‚    â”œâ”€ Convertit la liste de cours en DataFrame Pandas                  â”‚
â”‚    â””â”€ Exporte en fichier CSV avec encodage UTF-8                       â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

================================================================================
ğŸ’¡ POINTS CLÃ‰S Ã€ RETENIR
================================================================================

âœ“ PROGRAMMATION ASYNCHRONE : Utilise async/await pour ne pas bloquer le
  programme pendant les opÃ©rations longues (chargement de pages)

âœ“ ROBUSTESSE : Utilise plusieurs sÃ©lecteurs CSS pour chaque Ã©lÃ©ment car la
  structure HTML de Coursera peut varier

âœ“ GESTION D'ERREURS : Utilise try/except partout pour Ã©viter que le programme
  plante si un Ã©lÃ©ment n'est pas trouvÃ©

âœ“ RESPECT DU SERVEUR : Ajoute des dÃ©lais (REQUEST_DELAY) entre les requÃªtes
  pour ne pas surcharger le serveur Coursera

âœ“ SIMULATION DE NAVIGATEUR RÃ‰EL : Utilise un User-Agent et une rÃ©solution
  rÃ©aliste pour Ã©viter d'Ãªtre dÃ©tectÃ© comme un bot

âœ“ LAZY LOADING : Fait dÃ©filer la page pour charger le contenu dynamique
  qui ne s'affiche qu'au scroll

âœ“ EXPRESSIONS RÃ‰GULIÃˆRES : Utilise regex pour extraire des nombres et des
  patterns complexes dans le texte (notes, prix, reviews)

================================================================================
FIN DE L'EXPLICATION
================================================================================
