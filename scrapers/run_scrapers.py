"""
Run All Scrapers - Execute Coursera and Udemy scrapers
Combines results into a single dataset
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import asyncio
import pandas as pd
from datetime import datetime

from coursera_scraper import CourseraScraper
from udemy_scraper import UdemyScraper, generate_sample_udemy_data

try:
    from config import CATEGORIES, MAX_COURSES_PER_CATEGORY, HEADLESS_MODE, RAW_DATA_PATH
except ImportError:
    CATEGORIES = ['data-science', 'machine-learning', 'python', 'web-development', 'mathematics', 'statistics', 'economics', 'business', 'finance', 'marketing', 'AI', 'french', 'english', 'spanish', 'german', 'physics', 'chemistry', 'biology', 'geology', 'astronomy']
    MAX_COURSES_PER_CATEGORY = 200
    HEADLESS_MODE = True
    RAW_DATA_PATH = 'data/courses_raw.csv'


async def run_coursera_scraper(categories, max_per_category):
    """Ex√©cute le scraper Coursera"""
    scraper = CourseraScraper(headless=HEADLESS_MODE)
    courses = await scraper.scrape_all(categories, max_per_category)
    return courses


def run_udemy_scraper(categories, max_per_category):
    """Ex√©cute le scraper Udemy"""
    scraper = UdemyScraper()
    courses = scraper.scrape_all(categories, max_per_category)
    
    # Si peu de r√©sultats, compl√©ter avec des donn√©es d'exemple
    if len(courses) < len(categories) * 5:
        print("\n‚ö†Ô∏è Compl√©tant avec des donn√©es d'exemple Udemy...")
        sample_courses = generate_sample_udemy_data(categories, max_per_category)
        courses.extend(sample_courses)
        
    return courses


def combine_datasets(coursera_courses, udemy_courses):
    """Combine les datasets des deux plateformes"""
    all_courses = []
    
    # Ajouter les cours Coursera
    for i, course in enumerate(coursera_courses):
        course['id'] = f"coursera_{i+1}"
        all_courses.append(course)
        
    # Ajouter les cours Udemy
    for i, course in enumerate(udemy_courses):
        course['id'] = f"udemy_{i+1}"
        all_courses.append(course)
        
    return all_courses


def save_combined_dataset(courses, filepath):
    """Sauvegarde le dataset combin√©"""
    df = pd.DataFrame(courses)
    
    # Assurer l'ordre des colonnes
    columns_order = [
        'id', 'platform', 'title', 'description', 'category', 'skills',
        'instructor', 'rating', 'num_reviews', 'price', 'level',
        'language', 'duration', 'url', 'image_url', 'scraped_at'
    ]
    
    # Ajouter les colonnes manquantes
    for col in columns_order:
        if col not in df.columns:
            df[col] = ''
            
    # R√©organiser
    available_cols = [c for c in columns_order if c in df.columns]
    df = df[available_cols]
    
    # Sauvegarder
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    df.to_csv(filepath, index=False, encoding='utf-8')
    
    print(f"\nüíæ Dataset sauvegard√©: {filepath}")
    print(f"   üìä Total: {len(df)} cours")
    print(f"   üìä Coursera: {len(df[df['platform'] == 'Coursera'])} cours")
    print(f"   üìä Udemy: {len(df[df['platform'] == 'Udemy'])} cours")
    
    return df


async def main():
    """Fonction principale"""
    print("\n" + "="*70)
    print("   üöÄ COURSE RECOMMENDATION SYSTEM - DATA SCRAPING")
    print("="*70)
    print(f"\nüìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üìÇ Cat√©gories: {len(CATEGORIES)}")
    print(f"üì¶ Max cours/cat√©gorie: {MAX_COURSES_PER_CATEGORY}")
    
    # ===== COURSERA =====
    print("\n" + "-"*50)
    print("   √âtape 1: Scraping Coursera")
    print("-"*50)
    
    try:
        coursera_courses = await run_coursera_scraper(CATEGORIES, MAX_COURSES_PER_CATEGORY)
    except Exception as e:
        print(f"‚ùå Erreur Coursera: {e}")
        coursera_courses = []
        
    # ===== UDEMY =====
    print("\n" + "-"*50)
    print("   √âtape 2: Scraping Udemy")
    print("-"*50)
    
    try:
        udemy_courses = run_udemy_scraper(CATEGORIES[:6], MAX_COURSES_PER_CATEGORY)
    except Exception as e:
        print(f"‚ùå Erreur Udemy: {e}")
        udemy_courses = []
        
    # ===== COMBINER =====
    print("\n" + "-"*50)
    print("   √âtape 3: Combinaison des datasets")
    print("-"*50)
    
    all_courses = combine_datasets(coursera_courses, udemy_courses)
    
    if len(all_courses) == 0:
        print("‚ùå Aucun cours scrap√©!")
        print("   G√©n√©ration de donn√©es d'exemple...")
        
        # G√©n√©rer des donn√©es d'exemple pour les deux plateformes
        from udemy_scraper import generate_sample_udemy_data
        udemy_sample = generate_sample_udemy_data(CATEGORIES, MAX_COURSES_PER_CATEGORY)
        
        # Cr√©er des donn√©es Coursera similaires
        coursera_sample = []
        for i, course in enumerate(udemy_sample):
            coursera_course = course.copy()
            coursera_course['platform'] = 'Coursera'
            coursera_course['title'] = course['title'].replace('Bootcamp', 'Specialization')
            coursera_course['id'] = f"coursera_{i+1}"
            coursera_course['price'] = 'Subscription'
            coursera_sample.append(coursera_course)
            
        all_courses = udemy_sample + coursera_sample
        
    # Sauvegarder
    df = save_combined_dataset(all_courses, RAW_DATA_PATH)
    
    # Afficher un aper√ßu
    print("\nüìä Aper√ßu du dataset:")
    print(df.head(10).to_string())
    
    print("\n" + "="*70)
    print("   ‚úÖ SCRAPING TERMIN√â")
    print("="*70)
    
    return df


if __name__ == "__main__":
    asyncio.run(main())
