"""
Run All Scrapers - Execute Coursera and Udemy scrapers
Combines results into a single dataset
"""

import sys
import os
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import asyncio
import pandas as pd
from datetime import datetime

from coursera_scraper import CourseraScraper

try:
    from config import CATEGORIES, MAX_COURSES_PER_CATEGORY, HEADLESS_MODE, RAW_DATA_PATH
except ImportError:
    CATEGORIES = [
        'data-science', 
        'machine-learning', 
        'python', 
        'web-development', 
        'mathematics', 
        'statistics', 
        'economics', 
        'business', 
        'finance', 
        'marketing', 
        'AI', 
        'french', 
        'english', 
        'spanish', 
        'german', 
        'physics', 
        'chemistry', 
        'biology', 
        'geology', 
        'astronomy']
    MAX_COURSES_PER_CATEGORY = 200
    HEADLESS_MODE = True
    RAW_DATA_PATH = 'data/courses_raw.csv'


async def run_coursera_scraper(categories, max_per_category):
    """Ex√©cute le scraper Coursera"""
    scraper = CourseraScraper(headless=HEADLESS_MODE)
    courses = await scraper.scrape_all(categories, max_per_category)
    return courses








def save_combined_dataset(courses, filepath):
    """Sauvegarde le dataset combin√©"""
    df = pd.DataFrame(courses)
    
    # Assurer l'ordre des colonnes
    columns_order = [
        'id', 'platform', 'title', 'description', 'category', 'skills',
        'instructor', 'rating', 'num_reviews', 'price', 'level',
        'language', 'duration', 'url', 'image_url', 'scraped_at'
    ]
    
    # Ajouter les colonnes manquantes
    for col in columns_order:
        if col not in df.columns:
            df[col] = ''
            
    # R√©organiser
    available_cols = [c for c in columns_order if c in df.columns]
    df = df[available_cols]
    
    # Sauvegarder
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    df.to_csv(filepath, index=False, encoding='utf-8')
    
    print(f"\nüíæ Dataset sauvegard√©: {filepath}")
    print(f"   üìä Total: {len(df)} cours")
    print(f"   üìä Coursera: {len(df[df['platform'] == 'Coursera'])} cours")
    print(f"   üìä Udemy: {len(df[df['platform'] == 'Udemy'])} cours")
    
    return df


async def main():
    """Fonction principale"""
    print("\n" + "="*70)
    print("   üöÄ COURSE RECOMMENDATION SYSTEM - DATA SCRAPING (COURSERA ONLY)")
    print("="*70)
    print(f"\nüìÖ Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"üìÇ Cat√©gories: {len(CATEGORIES)}")
    print(f"üì¶ Max cours/cat√©gorie: {MAX_COURSES_PER_CATEGORY}")
    
    # ===== COURSERA =====
    print("\n" + "-"*50)
    print("   √âtape 1: Scraping Coursera")
    print("-"*50)
    
    try:
        coursera_courses = await run_coursera_scraper(CATEGORIES, MAX_COURSES_PER_CATEGORY)
    except Exception as e:
        print(f"‚ùå Erreur Coursera: {e}")
        coursera_courses = []
        
    if len(coursera_courses) == 0:
        print("‚ùå Aucun cours scrap√©!")
    else:
        # Sauvegarder uniquement Coursera
        # Utiliser RAW_DATA_PATH ou un nouveau chemin
        # Pour √©viter de casser la compatibilit√©, on sauvegarde dans coursera_raw.csv
        # ou on garde RAW_DATA_PATH en sachant qu'il n'y aura que du Coursera
        
        df = save_combined_dataset(coursera_courses, RAW_DATA_PATH)
    
    print("\n" + "="*70)
    print("   ‚úÖ SCRAPING COURSERA TERMIN√â")
    print("="*70)
    
    return coursera_courses


if __name__ == "__main__":
    asyncio.run(main())
