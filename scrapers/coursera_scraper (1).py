"""
Scraper Coursera - Extraire les cours de Coursera
Utilise Playwright pour le chargement de contenu dynamique
"""

# === IMPORTATION DES MODULES SYST√àME ===
import sys  # Module syst√®me pour acc√©der aux param√®tres Python
import os   # Module pour interagir avec le syst√®me d'exploitation (fichiers, chemins)
# Ajouter le r√©pertoire parent au chemin de recherche des modules
sys.path.insert(0, os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

# === IMPORTATION DES BIBLIOTH√àQUES PRINCIPALES ===
import asyncio  # Biblioth√®que pour la programmation asynchrone (async/await)
import pandas as pd  # Biblioth√®que pour la manipulation de donn√©es (DataFrames, CSV)
from datetime import datetime  # Pour obtenir la date et l'heure actuelles
from playwright.async_api import async_playwright  # Automatisation de navigateur pour contenu dynamique
import re  # Module pour les expressions r√©guli√®res (recherche de patterns dans le texte)
import time  # Module pour g√©rer le temps et les pauses

# === IMPORTATION DE LA CONFIGURATION ===
# Essayer d'importer les param√®tres depuis le fichier config.py
try:
    from config import CATEGORIES, MAX_COURSES_PER_CATEGORY, HEADLESS_MODE, REQUEST_DELAY
except ImportError:
    # Si le fichier config.py n'existe pas, utiliser des valeurs par d√©faut
    CATEGORIES = ['data-science', 'machine-learning', 'python']  # Cat√©gories √† scraper
    MAX_COURSES_PER_CATEGORY = 30  # Nombre maximum de cours par cat√©gorie
    HEADLESS_MODE = True  # Mode sans interface graphique (True = invisible)
    REQUEST_DELAY = 2  # D√©lai en secondes entre les requ√™tes (√©vite la surcharge)


# === CLASSE PRINCIPALE DU SCRAPER COURSERA ===
class CourseraScraper:
    """Scraper pour Coursera utilisant Playwright"""
    
    # Attributs de classe (partag√©s par toutes les instances)
    BASE_URL = "https://www.coursera.org"  # URL de base du site Coursera
    SEARCH_URL = "https://www.coursera.org/search?query={query}"  # Template d'URL de recherche
    
    def __init__(self, headless=True):
        """Constructeur de la classe - initialise une nouvelle instance du scraper"""
        self.headless = headless  # Mode d'affichage du navigateur (True = invisible)
        self.courses = []  # Liste vide pour stocker les cours extraits
        
    async def init_browser(self):
        """Initialise le navigateur Playwright"""
        # D√©marrer Playwright (async = op√©ration asynchrone, await = attendre la fin)
        self.playwright = await async_playwright().start()
        # Lancer le navigateur Chromium (headless = sans interface graphique si True)
        self.browser = await self.playwright.chromium.launch(headless=self.headless)
        # Cr√©er un contexte de navigation (environnement isol√© avec ses propres cookies/cache)
        self.context = await self.browser.new_context(
            viewport={'width': 1920, 'height': 1080},  # R√©solution de la fen√™tre (Full HD)
            user_agent='Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'  # Simule un navigateur r√©el
        )
        # Ouvrir une nouvelle page dans le contexte
        self.page = await self.context.new_page()
        
    async def close_browser(self):
        """Ferme le navigateur"""
        await self.browser.close()
        await self.playwright.stop()
        
    async def handle_cookie_consent(self):
        """G√®re le popup de consentement cookies"""
        try:
            # Liste de s√©lecteurs CSS pour trouver le bouton d'acceptation des cookies
            consent_selectors = [
                'button:has-text("Accept")',  # Bouton contenant le texte "Accept"
                'button:has-text("Accepter")',  # Version fran√ßaise
                'button[data-testid="accept-cookies"]',  # Bouton avec attribut data-testid
                '#onetrust-accept-btn-handler'  # ID sp√©cifique du bouton OneTrust
            ]
            # Essayer chaque s√©lecteur jusqu'√† trouver le bouton
            for selector in consent_selectors:
                try:
                    btn = self.page.locator(selector)  # Localiser l'√©l√©ment
                    if await btn.count() > 0:  # V√©rifier si l'√©l√©ment existe
                        await btn.first.click()  # Cliquer sur le premier √©l√©ment trouv√©
                        await asyncio.sleep(1)  # Attendre 1 seconde apr√®s le clic
                        break  # Sortir de la boucle si succ√®s
                except:
                    pass  # Ignorer les erreurs et essayer le s√©lecteur suivant
        except:
            pass  # Ignorer si aucun popup de cookies n'est trouv√©
            
    async def scroll_page(self, scroll_count=5):
        """Fait d√©filer la page pour charger plus de contenu (lazy loading)"""
        # Boucle pour d√©filer plusieurs fois (scroll_count = nombre de d√©filements)
        for _ in range(scroll_count):
            # Ex√©cuter du JavaScript dans la page pour d√©filer d'une hauteur d'√©cran
            # window.scrollBy(x, y) : d√©filer de x pixels horizontalement et y verticalement
            # window.innerHeight : hauteur de la fen√™tre du navigateur
            await self.page.evaluate('window.scrollBy(0, window.innerHeight)')
            await asyncio.sleep(1)  # Attendre 1 seconde pour laisser le contenu se charger
            
    def extract_text(self, element, default=''):
        """Extrait le texte d'un √©l√©ment de mani√®re s√©curis√©e"""
        try:
            return element.strip() if element else default
        except:
            return default
            
    async def scrape_category(self, category, max_courses=30):
        """Scrape les cours d'une cat√©gorie"""
        print(f"\nüîç Scraping Coursera: {category}")
        
        url = self.SEARCH_URL.format(query=category.replace('-', ' '))
        
        try:
            await self.page.goto(url, wait_until='networkidle', timeout=30000)
            await self.handle_cookie_consent()
            await asyncio.sleep(3)
            await self.scroll_page(5)
            
            # S√©lecteurs pour les cartes de cours
            card_selectors = [
                'li.cds-9.css-0.cds-11.cds-grid-item',
                'li.cds-9',
                'div[data-testid="search-result-card"]',
                'li[class*="ais-Hits-item"]'
            ]
            
            cards = []
            for selector in card_selectors:
                cards = await self.page.locator(selector).all()
                if len(cards) > 0:
                    print(f"   ‚úÖ Trouv√© {len(cards)} cours avec s√©lecteur: {selector[:30]}...")
                    break
                    
            if len(cards) == 0:
                print(f"   ‚ö†Ô∏è Aucun cours trouv√© pour {category}")
                return []
                
            courses = []
            for i, card in enumerate(cards[:max_courses]):
                try:
                    course_data = await self.extract_course_data(card, category)
                    if course_data and course_data.get('title'):
                        courses.append(course_data)
                except Exception as e:
                    continue
                    
            print(f"   üì¶ Extrait {len(courses)} cours pour {category}")
            return courses
            
        except Exception as e:
            print(f"   ‚ùå Erreur scraping {category}: {e}")
            return []
            
    async def extract_course_data(self, card, category):
        """Extrait les donn√©es d'une carte de cours"""
        # Cr√©er un dictionnaire pour stocker les informations du cours
        course = {
            'platform': 'Coursera',  # Nom de la plateforme
            'category': category.replace('-', ' ').title(),  # Cat√©gorie format√©e (ex: 'data-science' ‚Üí 'Data Science')
            'scraped_at': datetime.now().isoformat()  # Date/heure d'extraction au format ISO (ex: '2026-02-12T10:30:00')
        }
        
        # === EXTRACTION DU TITRE ===
        # Liste de s√©lecteurs CSS possibles pour trouver le titre (essayer dans l'ordre)
        title_selectors = ['h3', 'h2', '[data-testid="product-card-title"]', '.product-name']
        for sel in title_selectors:
            try:
                elem = card.locator(sel)  # Chercher l'√©l√©ment dans la carte
                if await elem.count() > 0:  # V√©rifier si l'√©l√©ment existe
                    course['title'] = await elem.first.inner_text()  # Extraire le texte du premier √©l√©ment
                    break  # Sortir de la boucle d√®s qu'on trouve le titre
            except:
                pass  # Ignorer les erreurs et essayer le s√©lecteur suivant
                
        # Organisation/Instructeur
        org_selectors = ['p.cds-ProductCard-partnerNames', 'span[data-testid="partner-names"]', '.partner-name']
        for sel in org_selectors:
            try:
                elem = card.locator(sel)
                if await elem.count() > 0:
                    course['instructor'] = await elem.first.inner_text()
                    break
            except:
                pass
                
        # === EXTRACTION DE LA NOTE (RATING) ===
        # Chercher les √©l√©ments contenant la note du cours
        rating_selectors = ['[aria-label*="rating"]', 'span:has-text("stars")', '.ratings-text']
        for sel in rating_selectors:
            try:
                elem = card.locator(sel)
                if await elem.count() > 0:
                    text = await elem.first.inner_text()  # Obtenir le texte (ex: "4.8 stars")
                    # Utiliser une expression r√©guli√®re pour extraire le nombre
                    # r'(\d+[.,]?\d*)' : cherche un nombre avec ou sans d√©cimales
                    match = re.search(r'(\d+[.,]?\d*)', text)
                    if match:
                        # Convertir en float (remplacer virgule par point pour format anglais)
                        course['rating'] = float(match.group(1).replace(',', '.'))
                    break
            except:
                pass
                
        # === EXTRACTION DU NOMBRE DE REVIEWS ===
        try:
            text = await card.inner_text()  # Obtenir tout le texte de la carte
            # Regex pour trouver le nombre de reviews (ex: "22,772 reviews" ou "150K students")
            # \(? : parenth√®se optionnelle, [\d.,]+ : nombre avec virgules/points
            # [KkMm]? : suffixe K ou M optionnel, re.I : ignorer la casse
            review_match = re.search(r'\(?([\d.,]+)\s*[KkMm]?\)?\s*(?:reviews?|avis|√©tudiants?|students?)', text, re.I)
            if review_match:
                num_str = review_match.group(1).replace(',', '.')  # Extraire le nombre
                num = float(num_str)
                # G√©rer les suffixes K (milliers) et M (millions)
                if 'k' in text.lower():
                    num *= 1000  # Multiplier par 1000 si K
                elif 'm' in text.lower():
                    num *= 1000000  # Multiplier par 1 million si M
                course['num_reviews'] = int(num)  # Convertir en entier
        except:
            pass  # Ignorer si non trouv√©
            
        # === EXTRACTION DU NIVEAU DU COURS ===
        # Dictionnaire de mots-cl√©s pour identifier le niveau
        level_keywords = {
            'Beginner': ['d√©butant', 'beginner', 'introduct', 'basic'],  # Mots pour niveau d√©butant
            'Intermediate': ['interm√©diaire', 'intermediate', 'medium'],  # Mots pour niveau interm√©diaire
            'Advanced': ['avanc√©', 'advanced', 'expert', 'professional']  # Mots pour niveau avanc√©
        }
        
        try:
            text = await card.inner_text()  # Obtenir tout le texte de la carte
            text_lower = text.lower()  # Convertir en minuscules pour comparaison
            # Parcourir chaque niveau et ses mots-cl√©s
            for level, keywords in level_keywords.items():
                # V√©rifier si un des mots-cl√©s est pr√©sent dans le texte
                # any() : retourne True si au moins un √©l√©ment est True
                if any(kw in text_lower for kw in keywords):
                    course['level'] = level  # Assigner le niveau trouv√©
                    break  # Sortir de la boucle d√®s qu'un niveau est trouv√©
        except:
            pass  # Ignorer si le niveau n'est pas trouv√©
            
        # === EXTRACTION DU PRIX ===
        try:
            text = await card.inner_text()  # Obtenir le texte de la carte
            # V√©rifier si le cours est gratuit (mots "free" ou "gratuit")
            if 'free' in text.lower() or 'gratuit' in text.lower():
                course['price'] = 'Free'  # Marquer comme gratuit
            else:
                # Chercher un prix avec symbole mon√©taire (ex: "$49.99", "‚Ç¨29.99")
                # [\$‚Ç¨¬£] : symboles de devises, \s* : espaces optionnels
                # \d+(?:[.,]\d{2})? : nombre avec optionnellement 2 d√©cimales
                price_match = re.search(r'[\$‚Ç¨¬£]\s*(\d+(?:[.,]\d{2})?)', text)
                if price_match:
                    course['price'] = price_match.group(0)  # Extraire le prix complet avec symbole
                else:
                    course['price'] = 'Subscription'  # Si pas de prix, c'est probablement un abonnement
        except:
            pass  # Ignorer si le prix n'est pas trouv√©
            
        # === EXTRACTION DE L'URL DU COURS ===
        try:
            link = card.locator('a').first  # Trouver le premier lien <a> dans la carte
            if await link.count() > 0:  # V√©rifier que le lien existe
                href = await link.get_attribute('href')  # Obtenir l'attribut href du lien
                if href:
                    # V√©rifier si l'URL est relative (commence par /) ou absolue
                    if href.startswith('/'):
                        course['url'] = self.BASE_URL + href  # Construire l'URL compl√®te
                    else:
                        course['url'] = href  # Utiliser l'URL telle quelle
        except:
            pass  # Ignorer si l'URL n'est pas trouv√©e
            
        # === EXTRACTION DE LA DESCRIPTION ===
        try:
            # Chercher les paragraphes qui ne sont pas le nom de l'organisation
            desc_selectors = ['p:not(.cds-ProductCard-partnerNames)', '.description']
            for sel in desc_selectors:
                elem = card.locator(sel)
                if await elem.count() > 0:
                    course['description'] = await elem.first.inner_text()
                    break
        except:
            # Si pas de description, utiliser le titre comme description par d√©faut
            course['description'] = course.get('title', '')
            
        # === INFORMATIONS SUPPL√âMENTAIRES ===
        # Comp√©tences : utiliser la cat√©gorie comme comp√©tences (remplacer - par ,)
        course['skills'] = category.replace('-', ', ')
        # Langue : par d√©faut anglais (Coursera est principalement en anglais)
        course['language'] = 'English'
        
        return course  # Retourner le dictionnaire contenant toutes les donn√©es du cours
        
    async def scrape_all(self, categories=None, max_per_category=30):
        """Scrape toutes les cat√©gories"""
        # Si aucune cat√©gorie n'est fournie, utiliser celles de la configuration
        if categories is None:
            categories = CATEGORIES
            
        # Afficher le header du programme
        print("\n" + "="*60)
        print("   SCRAPER COURSERA")
        print("="*60)
        
        # Initialiser le navigateur Playwright
        await self.init_browser()
        
        all_courses = []  # Liste pour stocker tous les cours de toutes les cat√©gories
        # Boucle sur chaque cat√©gorie avec enumerate pour avoir l'index
        for i, category in enumerate(categories):
            print(f"\n[{i+1}/{len(categories)}] Cat√©gorie: {category}")
            # Scraper la cat√©gorie actuelle
            courses = await self.scrape_category(category, max_per_category)
            # Ajouter les cours de cette cat√©gorie √† la liste totale
            # extend() : ajoute tous les √©l√©ments d'une liste √† une autre
            all_courses.extend(courses)
            # Attendre REQUEST_DELAY secondes avant la prochaine cat√©gorie (√©vite la surcharge)
            await asyncio.sleep(REQUEST_DELAY)
            
        # Fermer le navigateur apr√®s avoir termin√©
        await self.close_browser()
        
        # Stocker les cours dans l'attribut de l'instance
        self.courses = all_courses
        print(f"\n‚úÖ Total: {len(all_courses)} cours scrap√©s de Coursera")
        
        return all_courses  # Retourner la liste compl√®te des cours
        
    def to_dataframe(self):
        """Convertit les cours en DataFrame Pandas"""
        # pd.DataFrame() : cr√©e un tableau structur√© √† partir d'une liste de dictionnaires
        # Chaque dictionnaire devient une ligne, les cl√©s deviennent les colonnes
        return pd.DataFrame(self.courses)
        
    def save_to_csv(self, filepath):
        """Sauvegarde les cours dans un fichier CSV"""
        df = self.to_dataframe()  # Convertir en DataFrame
        # to_csv() : exporte le DataFrame en fichier CSV
        # index=False : ne pas inclure l'index des lignes
        # encoding='utf-8' : utiliser l'encodage UTF-8 pour supporter les caract√®res sp√©ciaux
        df.to_csv(filepath, index=False, encoding='utf-8')
        print(f"üíæ Sauvegard√©: {filepath}")


async def main():
    """Fonction principale pour tester le scraper"""
    scraper = CourseraScraper(headless=HEADLESS_MODE)
    
    # Scraper quelques cat√©gories pour le test
    test_categories = ['data-science', 'machine-learning', 'python']
    courses = await scraper.scrape_all(test_categories, max_per_category=20)
    
    if courses:
        scraper.save_to_csv('data/coursera_courses.csv')
        print(f"\nüìä Exemple de cours:")
        df = scraper.to_dataframe()
        print(df.head())
        
        
if __name__ == "__main__":
    asyncio.run(main())
